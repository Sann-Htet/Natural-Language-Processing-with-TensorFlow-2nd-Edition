{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question answering with BERT (HuggingFace)\n"
      ],
      "metadata": {
        "id": "zBUS3Pg8uO8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "QSDqlTPuu0vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q448dfunu-Rj",
        "outputId": "25e997fd-f17c-4eca-dede-e1f4584f9721"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertConfig, TFDistilBertForQuestionAnswering\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
        "    try:\n",
        "        np.random.seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
        "    try:\n",
        "        tf.random.set_seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
        "    try:\n",
        "        random.seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
        "    try:\n",
        "        transformers.trainer_utils.set_seed(seed)\n",
        "    except NameError:\n",
        "        print(\"Warning: transformers module is not imported. Setting the seed for transformers failed.\")\n",
        "\n",
        "# Fixing the random seed\n",
        "random_seed=4321\n",
        "set_random_seed(random_seed)\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "  print(\"No GPU found!\")\n",
        "  pass"
      ],
      "metadata": {
        "id": "PDuZ7dcuu3Qa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset\n",
        "\n",
        "For this we will be using the [SQUAD v1 dataset](https://rajpurkar.github.io/SQuAD-explorer/). It is a question answering dataset. We are provided with a question, a context (e.g. a paragraph in which the answer to the question may exist) and finally the answer. Our goal is to, given the question and the context predict the answer."
      ],
      "metadata": {
        "id": "rb3uLxdPvFDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad\")"
      ],
      "metadata": {
        "id": "7Vp_lYZuvWur"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deJ2IKySve5g",
        "outputId": "769ab685-eb1f-4680-cdf1-9d8445c128fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 87599\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 10570\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print the first 5 samples in the training set"
      ],
      "metadata": {
        "id": "oGafq8AWvirC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for q, a in zip(dataset[\"train\"][\"question\"][:5], dataset[\"train\"][\"answers\"][:5]):\n",
        "    print(f\"{q} -> {a}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSpi8iw-vlxq",
        "outputId": "4f90bb7c-062a-49b1-8873-f859c8fe6015"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? -> {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
            "What is in front of the Notre Dame Main Building? -> {'text': ['a copper statue of Christ'], 'answer_start': [188]}\n",
            "The Basilica of the Sacred heart at Notre Dame is beside to which structure? -> {'text': ['the Main Building'], 'answer_start': [279]}\n",
            "What is the Grotto at Notre Dame? -> {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]}\n",
            "What sits on top of the Main Building at Notre Dame? -> {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correcting incorrect offsets of the provided answers\n",
        "\n",
        "The answers are provided by means of the, starting index (`answer_start`) and the answer it self (`text`). We will add `answer_end`, which will denote the index of the position the answer ends."
      ],
      "metadata": {
        "id": "TBrZMPkHvoLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_end_index(answers, contexts):\n",
        "  \"\"\" Add end index to answers \"\"\"\n",
        "  fixed_answers = []\n",
        "  for answer, context in zip(answers, contexts):\n",
        "    gold_text = answer['text'][0]\n",
        "    answer['text'] = gold_text\n",
        "    start_idx = answer['answer_start'][0]\n",
        "    answer['answer_start'] = start_idx\n",
        "\n",
        "    # Make sure the starting index is valid and there is an answer\n",
        "    assert start_idx >= 0 and len(gold_text.strip()) > 0\n",
        "\n",
        "    end_idx = start_idx + len(gold_text)\n",
        "    answer['answer_end'] = end_idx\n",
        "\n",
        "    # Make sure the corresponding context matches the actual answer\n",
        "    assert context[start_idx:end_idx] == gold_text\n",
        "\n",
        "    fixed_answers.append(answer)\n",
        "  return fixed_answers, contexts\n",
        "\n",
        "train_questions = dataset[\"train\"][\"question\"]\n",
        "print(\"Training data corrections\")\n",
        "train_answers, train_contexts = compute_end_index(\n",
        "    dataset[\"train\"][\"answers\"], dataset[\"train\"][\"context\"]\n",
        ")\n",
        "test_questions = dataset[\"validation\"][\"question\"]\n",
        "print(\"\\nValidation data correction\")\n",
        "test_answers, test_contexts = compute_end_index(\n",
        "    dataset[\"validation\"][\"answers\"], dataset[\"validation\"][\"context\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F97dFZyFvrWR",
        "outputId": "18e59024-f011-4ead-ba7f-86df098fb887"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data corrections\n",
            "\n",
            "Validation data correction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question answering with Bert\n",
        "\n",
        "Now we will start our way to train a question answering model. The pretrained model we'll be using is [Bert](https://arxiv.org/pdf/1810.04805.pdf)."
      ],
      "metadata": {
        "id": "zEykoZFZwioy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the tokenizer"
      ],
      "metadata": {
        "id": "yYAjzh31wp1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "bsUCeEV3w7jA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert some text to tokens with the tokenizer"
      ],
      "metadata": {
        "id": "8rJhFX3nzH8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"This is the context\"\n",
        "question = \"This is the question\"\n",
        "\n",
        "token_ids = tokenizer(\n",
        "    text=context, text_pair=question, padding=False, return_tensors='tf'\n",
        ")\n",
        "\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fE2yorQzLio",
        "outputId": "e7f3da88-bb73-4efd-b7db-a27613f238e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=\n",
            "array([[ 101, 2023, 2003, 1996, 6123,  102, 2023, 2003, 1996, 3160,  102]],\n",
            "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(token_ids[\"input_ids\"].numpy()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtbXOenxzZ1g",
        "outputId": "1aed65e6-c746-47bc-cf27-b245fcf6f8d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'this', 'is', 'the', 'context', '[SEP]', 'this', 'is', 'the', 'question', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode train data\n",
        "train_encodings = tokenizer(train_contexts, train_questions,\n",
        "                            truncation=True, padding=True, return_tensors='tf')\n",
        "\n",
        "# Encode test data\n",
        "test_encodings = tokenizer(test_contexts, test_questions, truncation=True,\n",
        "                           padding=True, return_tensors='tf')"
      ],
      "metadata": {
        "id": "TOZZxR_FzjoG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_encoding.shape: {}\".format(train_encodings[\"input_ids\"].shape))\n",
        "print(f\"test_encodings.shape: {test_encodings['input_ids'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arIFnIsRz5i5",
        "outputId": "c55eb362-3f13-48c2-c882-c72d0f00ebc3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_encoding.shape: (87599, 512)\n",
            "test_encodings.shape: (10570, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with truncated answers\n",
        "\n",
        "In the original dataset the `answer_start` and `answer_end` denote the *character*-level position of the answer. But in the model, since we deal in tokens we need the *token*-level position of the answer. For that, we will use the `char_to_token` function in the tokenizer. It will convert the character index to a token index.\n",
        "\n",
        "Because we are enforcing a maximum sequence length of 512, some answers will be inevitably truncated if they are present after the 512th token. Although this is rare, we still need to take care of this as it can result in numerical errors otherwise. Therefore, if the positions are `None` (i.e. couldn't find the answer), it is set to the maximum position."
      ],
      "metadata": {
        "id": "GFsW36AJ0AtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_char_with_token_indices(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    n_updates = 0\n",
        "    # Go through all the answers\n",
        "    for i in range(len(answers)):\n",
        "\n",
        "        # Get the token position for both start end char positions\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "\n",
        "        if start_positions[-1] is None or end_positions[-1] is None:\n",
        "            n_updates += 1\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        # In the guide, https://huggingface.co/transformers/custom_datasets.html#qa-squad\n",
        "        # they set it to model_max_length, but this will result in NaN losses as the last\n",
        "        # available label is model_max_length-1 (zero-indexed)\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length -1\n",
        "\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length -1\n",
        "\n",
        "    print(f\"{n_updates}/{len(answers)} had answers truncated\")\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "replace_char_with_token_indices(train_encodings, train_answers)\n",
        "replace_char_with_token_indices(test_encodings, test_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvpI05NZ1_E2",
        "outputId": "476057f9-e25b-4998-8921-0ab24892251c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/87599 had answers truncated\n",
            "8/10570 had answers truncated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating TensorFlow dataset"
      ],
      "metadata": {
        "id": "LwswmZn22B38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "\n",
        "train_batch_size = 4\n",
        "test_batch_size = 8\n",
        "\n",
        "def data_gen(input_ids, attention_mask, start_positions, end_positions):\n",
        "    \"\"\" Generator for data \"\"\"\n",
        "    for inps, attn, start_pos, end_pos in zip(input_ids, attention_mask, start_positions, end_positions):\n",
        "\n",
        "        yield (inps, attn), (start_pos, end_pos)\n",
        "\n",
        "print(\"Creating train data\")\n",
        "\n",
        "# Define the generator as a callable (not the generator it self)\n",
        "train_data_gen = partial(data_gen,\n",
        "    input_ids=train_encodings['input_ids'], attention_mask=train_encodings['attention_mask'],\n",
        "    start_positions=train_encodings['start_positions'], end_positions=train_encodings['end_positions']\n",
        ")\n",
        "\n",
        "# Define the dataset\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    train_data_gen, output_types=(('int32', 'int32'), ('int32', 'int32'))\n",
        ")\n",
        "# Shuffling the data\n",
        "train_dataset = train_dataset.shuffle(1000)\n",
        "print('\\tDone')\n",
        "\n",
        "# Valid set is taken as the first 10000 samples in the shuffled set\n",
        "valid_dataset = train_dataset.take(10000)\n",
        "valid_dataset = valid_dataset.batch(train_batch_size)\n",
        "\n",
        "# Rest is kept as the training data\n",
        "train_dataset = train_dataset.skip(10000)\n",
        "train_dataset = train_dataset.batch(train_batch_size)\n",
        "\n",
        "# Creating test data\n",
        "print(\"Creating test data\")\n",
        "\n",
        "# Define the generator as a callable\n",
        "test_data_gen = partial(data_gen,\n",
        "    input_ids=test_encodings['input_ids'], attention_mask=test_encodings['attention_mask'],\n",
        "    start_positions=test_encodings['start_positions'], end_positions=test_encodings['end_positions']\n",
        ")\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    test_data_gen, output_types=(('int32', 'int32'), ('int32', 'int32'))\n",
        ")\n",
        "test_dataset = test_dataset.batch(test_batch_size)\n",
        "print(\"\\tDone\")"
      ],
      "metadata": {
        "id": "sYnMH5HF2GNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef8bd57-5b4f-4a44-b4e6-f632a6f1b9e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating train data\n",
            "\tDone\n",
            "Creating test data\n",
            "\tDone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the model\n",
        "\n",
        "Here we define a DistilBert model (particularly a TF variant)"
      ],
      "metadata": {
        "id": "TlYxQIicpU8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, TFBertForQuestionAnswering\n",
        "\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\", return_dict=False)\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8rrofuHpZeQ",
        "outputId": "dd8a33d1-4cf0-47ae-f83c-bd6b27cf8884"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"return_dict\": false,\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\", config=config)\n",
        "\n",
        "def tf_wrap_model(model):\n",
        "    \"\"\" Wraps the huggingface's model with in the Keras Functional API \"\"\"\n",
        "\n",
        "    # If this is not wrapped in a keras model by taking the correct tensors from\n",
        "    # TFQuestionAnsweringModelOutput produced, you will get the following error\n",
        "    # setting return_dict did not seem to work as it should\n",
        "\n",
        "    # TypeError: The two structures don't have the same sequence type.\n",
        "    # Input structure has type <class 'tuple'>, while shallow structure has type\n",
        "    # <class 'transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput'>.\n",
        "\n",
        "    # Define inputs\n",
        "    input_ids = tf.keras.layers.Input([None,], dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = tf.keras.layers.Input([None,], dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "    # Define the output (TFQuestionAnsweringModelOutput)\n",
        "    out = model([input_ids, attention_mask])\n",
        "\n",
        "    # Get the correct attributes in the produced object to generate an output tuple\n",
        "    wrap_model = tf.keras.models.Model([input_ids, attention_mask], outputs=(out[0], out[1]))\n",
        "\n",
        "    return wrap_model\n",
        "\n",
        "\n",
        "# Define and compile the model\n",
        "\n",
        "# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss\n",
        "# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.\n",
        "# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "\n",
        "model_v2 = tf_wrap_model(model)\n",
        "model_v2.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc4_EYIErpY2",
        "outputId": "a8cdac0b-c58c-4dcd-8b7b-3736c960e6b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "yWPk37uUspAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "model_v2.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=2\n",
        ")\n",
        "\n",
        "t2 = time.time()\n",
        "\n",
        "print(f\"It took {t2-t1} seconds to complete the training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hrMvWN22tMEE",
        "outputId": "9576298a-f84b-4da7-8b55-19828a3c0824"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-04f64fcaa55b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model_v2.fit(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'tf_bert_for_question_answering' (type TFBertForQuestionAnswering).\n    \n    in user code:\n    \n        File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1833, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 1852, in call  *\n            outputs = self.bert(\n        File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1833, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 766, in call  *\n                batch_size, seq_length = input_shape\n        \n            OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n        \n        \n        Call arguments received by layer 'bert' (type TFBertMainLayer):\n          • input_ids=tf.Tensor(shape=<unknown>, dtype=int32)\n          • attention_mask=tf.Tensor(shape=<unknown>, dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=False\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_bert_for_question_answering' (type TFBertForQuestionAnswering):\n      • input_ids=['tf.Tensor(shape=<unknown>, dtype=int32)', 'tf.Tensor(shape=<unknown>, dtype=int32)']\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • start_positions=None\n      • end_positions=None\n      • training=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the model"
      ],
      "metadata": {
        "id": "4METxV4itUWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_v2.summary())"
      ],
      "metadata": {
        "id": "pl8a5BIAtXhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: We cannot save `model_v2` as is, because it raises an error about not finding config for the transformer model layer. THerefore, we will save just the transformer model layer, so that we can call the `tf_wrap_model()` function anytime and get the wrapped model."
      ],
      "metadata": {
        "id": "PXVljVcMtbIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create folders\n",
        "if not os.path.exists('models'):\n",
        "  os.makedirs('models')\n",
        "if not os.path.exists('tokenizers'):\n",
        "  os.makedirs('tokenizers')\n",
        "\n",
        "# Save the model\n",
        "model_v2.get_layer(\"tf_bert_for_question_answering\").save_pretrained(os.path.join('models', 'bert_qa'))\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(os.path.join('tokenizers', 'bert_qa'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tVvgpcrthPi",
        "outputId": "c9902eb6-2aec-4091-c028-5d62ed8b8690"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tokenizers/bert_qa/tokenizer_config.json',\n",
              " 'tokenizers/bert_qa/special_tokens_map.json',\n",
              " 'tokenizers/bert_qa/vocab.txt',\n",
              " 'tokenizers/bert_qa/added_tokens.json',\n",
              " 'tokenizers/bert_qa/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on unseen data"
      ],
      "metadata": {
        "id": "Hx0Rgr2JuUdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_v2.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "nISn9xzVuXBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ask BERT a question ..."
      ],
      "metadata": {
        "id": "M19GmK5lueta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 7\n",
        "\n",
        "# Define sample question\n",
        "sample_q = test_questions[i]\n",
        "# Define sample context\n",
        "sample_c = test_contexts[i]\n",
        "# Define sample answer\n",
        "sample_a = test_answers[i]\n",
        "\n",
        "# Get the input in the format BERT accepts\n",
        "sample_input = (test_encodings[\"input_ids\"][i:i+1], test_encodings[\"attention_mask\"][i:i+1])\n",
        "\n",
        "def ask_bert(sample_input, tokenizer, model):\n",
        "    \"\"\" This function takes an input, a tokenizer, a model and returns the prediciton \"\"\"\n",
        "    out = model.predict(sample_input)\n",
        "    pred_ans_start = tf.argmax(out[0][0])\n",
        "    pred_ans_end = tf.argmax(out[1][0])\n",
        "    print(f\"{pred_ans_start}-{pred_ans_end} token ids contain the answer\")\n",
        "    ans_tokens = sample_input[0][0][pred_ans_start:pred_ans_end+1]\n",
        "\n",
        "    return \" \".join(tokenizer.convert_ids_to_tokens(ans_tokens))\n",
        "\n",
        "print(\"Question\")\n",
        "print(\"\\t\", sample_q, \"\\n\")\n",
        "print(\"Context\")\n",
        "print(\"\\t\", sample_c, \"\\n\")\n",
        "print(\"Answer (char indexed)\")\n",
        "print(\"\\t\", sample_a, \"\\n\")\n",
        "print('='*50,'\\n')\n",
        "\n",
        "sample_pred_ans = ask_bert(sample_input, tokenizer, model_v2)\n",
        "\n",
        "print(\"Answer (predicted)\")\n",
        "print(sample_pred_ans)\n",
        "print('='*50,'\\n')"
      ],
      "metadata": {
        "id": "Ti3PPiGDuh4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "EwjDoWspyRQm"
      }
    }
  ]
}