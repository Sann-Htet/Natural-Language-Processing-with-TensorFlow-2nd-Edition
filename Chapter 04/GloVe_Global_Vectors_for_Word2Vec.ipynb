{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ksLMO7rEE0lY"
      },
      "outputs": [],
      "source": [
        "from six.moves.urllib.request import urlretrieve\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from matplotlib import pylab\n",
        "from scipy.sparse import lil_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the data\n",
        "### Downloading the data\n",
        "This code downloads a BBC dataset consisting of news articles published by BBC."
      ],
      "metadata": {
        "id": "ew61isQ2LdgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
        "\n",
        "def download_data(url, data_dir):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "\n",
        "    # Create the data directory if not exist\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
        "\n",
        "    # If file doesnt exist, download\n",
        "    if not os.path.exists(file_path):\n",
        "        print('Downloading file...')\n",
        "        filename, _ = urlretrieve(url, file_path)\n",
        "    else:\n",
        "        print(\"File already exists\")\n",
        "\n",
        "    extract_path = os.path.join(data_dir, 'bbc')\n",
        "\n",
        "    # If data has not been extracted already, extract data\n",
        "    if not os.path.exists(extract_path):\n",
        "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
        "            zipf.extractall(data_dir)\n",
        "    else:\n",
        "        print(\"bbc-fulltext.zip has already been extracted\")\n",
        "\n",
        "download_data(url, 'data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wltEPCcFLVMI",
        "outputId": "29787c4e-b93e-40ab-9b47-1cfb10b1e8fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Data without Preprocessing\n",
        "Here we read all the files and keep them as a list of strings, where each string is a single article"
      ],
      "metadata": {
        "id": "uQhi5W6BL0Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data_dir):\n",
        "    # This will contain the full list of stories\n",
        "    news_stories = []\n",
        "\n",
        "    print(\"Reading files\")\n",
        "\n",
        "    i = 0 # Just used for printing progress\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for fi, f in enumerate(files):\n",
        "            # We don't read the readme file\n",
        "            if 'README' in f:\n",
        "                continue\n",
        "\n",
        "            # Printing progress\n",
        "            i += 1\n",
        "            print(\".\"*i, f, end='\\r')\n",
        "\n",
        "            # Open the file\n",
        "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
        "                story = []\n",
        "                # Read all the lines\n",
        "                for row in f:\n",
        "                    story.append(row.strip())\n",
        "\n",
        "                # Create a single string with all the rows in the doc\n",
        "                story = ' '.join(story)\n",
        "                # Add that to the list\n",
        "                news_stories.append(story)\n",
        "\n",
        "        print('', end='\\r')\n",
        "\n",
        "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
        "    return news_stories\n",
        "\n",
        "\n",
        "news_stories = read_data(os.path.join('data', 'bbc'))\n",
        "\n",
        "# Printing some stats and sample data\n",
        "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
        "print('Example words (start): ',news_stories[0][:50])\n",
        "print('Example words (end): ',news_stories[-1][-50:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sGfeKryLm9q",
        "outputId": "e0ff7ca3-dc87-4943-b120-19cdcc2be460"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files\n",
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 425.txt\n",
            "Detected 2225 stories\n",
            "865163 words found in the total news set\n",
            "Example words (start):  Lennon brands Rangers favourites  Celtic's Neil Le\n",
            "Example words (end):  er the circumstances that led to the restatement.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a Tokenizer\n",
        "Here we build a tokenizer, that performs simple preprocessing like,\n",
        "\n",
        "- Converting letters to lower case\n",
        "- Removing punctuation\n",
        "\n",
        "and tokenize the strings based on a defined separator. Then each token is converted to an Integer ID, as computers understand numbers, not strings. In the background, the tokenizer builds a word to index dictionary, that defines a unique ID for each word in the vocabulary."
      ],
      "metadata": {
        "id": "46HW1ZDBMP2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "n_vocab = 15000 + 1\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=n_vocab - 1,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, split=' ', oov_token=''\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsgQJO2PMApE",
        "outputId": "523389f7-eb04-41af-e876-1bf6831a95ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the word co-occurrence matrix\n",
        "Why GloVe shine above context window based method is that it employs global statistics of the corpus in to the model (according to authors). This is done by using information from the word co-occurance matrix to optimize the word vectors. Basically, the $X(i, j)$\n",
        " entry of the co-occurance matrix says how frequent word $i$\n",
        " to appear near $j$.\n",
        "\n",
        "We also use an optional weighting mechanishm to give more weight to words close together than to the ones further-apart (from experiments section of the paper).\n",
        "\n",
        "**Note:** When generating the matrix for the first time, it will take a significant amount of time to run"
      ],
      "metadata": {
        "id": "ZGpEqzYVMoKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import save_npz, load_npz\n",
        "\n",
        "def generate_cooc_matrix(text, tokenizer, window_size, n_vocab, use_weighting=True):\n",
        "    # Convert list of text to list of list of word IDs\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "    # A sparse matrix to retain co-occurrences of words\n",
        "    cooc_mat = lil_matrix((n_vocab, n_vocab), dtype=np.float32)\n",
        "\n",
        "    # Go through each sequence one by one\n",
        "    for si, sequence in enumerate(sequences):\n",
        "\n",
        "        # Printing the progress\n",
        "        if (si+1)%100==0:\n",
        "            print('.'*((si+1)//100), f\"{si+1}/{len(sequences)}\", end='\\r')\n",
        "\n",
        "        # For each target word,\n",
        "        for i, wi in zip(np.arange(window_size, len(sequence)-window_size), sequence[window_size:-window_size]):\n",
        "\n",
        "            # Get the context window word IDs\n",
        "            context_window = sequence[i-window_size: i+window_size+1]\n",
        "\n",
        "            # The weight for the words in the context window (except target word) will be 1\n",
        "            window_weights = np.ones(shape=(window_size*2 + 1,), dtype=np.float32)\n",
        "            window_weights[window_size] = 0.0\n",
        "\n",
        "            if use_weighting:\n",
        "                # If weighting is used, penalize context words based on distance to target word\n",
        "                distances = np.abs(np.arange(-window_size, window_size+1))\n",
        "                distances[window_size] = 1.0\n",
        "                # Update the sparse matrix\n",
        "                cooc_mat[wi, context_window] += window_weights/distances\n",
        "            else:\n",
        "                # Update the sparse matrix\n",
        "                cooc_mat[wi, context_window] += window_weights\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return cooc_mat\n",
        "\n",
        "# ----------------------------------------- IMPORTANT ---------------------------------------------- #\n",
        "#                                                                                                    #\n",
        "# Set this true or false, depending on whether you want to generate the matrix or reuse the existing #\n",
        "#                                                                                                    #\n",
        "# ---------------------------------------------------------------------------------------------------#\n",
        "generate_cooc = True\n",
        "\n",
        "# Generate the matrix\n",
        "if generate_cooc:\n",
        "    t1 = time.time()\n",
        "    cooc_mat = generate_cooc_matrix(news_stories, tokenizer, 1, n_vocab, True)\n",
        "    t2 = time.time()\n",
        "    print(f\"It took {t2-t1} seconds to generate the co-occurrence matrix\")\n",
        "\n",
        "    save_npz(os.path.join('data','cooc_mat.npz'), cooc_mat.tocsr())\n",
        "# Load the matrix from disk\n",
        "else:\n",
        "    try:\n",
        "        cooc_mat = load_npz(os.path.join('data','cooc_mat.npz')).tolil()\n",
        "        print(f\"Cooc matrix of type {type(cooc_mat).__name__} was loaded from disk\")\n",
        "    except FileNotFoundError as ex:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not find the co-occurrence matrix on the disk. Did you generate the matrix by setting generate_cooc=True?\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBa9gB9lMj1V",
        "outputId": "e7a01b51-c8b9-46c9-be85-9c34c80972aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "It took 848.7204871177673 seconds to generate the co-occurrence matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the validity of the co-occurrence matrix\n",
        "Here we will see if the context around a given word has sensible words appearing in it"
      ],
      "metadata": {
        "id": "YB1L5OIoOMq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "word = 'sport'\n",
        "assert word in tokenizer.word_index, f\"Word {word} is not in the tokenizer\"\n",
        "assert tokenizer.word_index[word] <= n_vocab, f\"The word {word} is an out of vocabuary word. Please try something else\"\n",
        "\n",
        "# Get the vector of co-occurrences for a given word\n",
        "cooc_vec = np.array(cooc_mat.getrow(tokenizer.word_index[word]).todense()).ravel()\n",
        "# Get indices of words with maximum value\n",
        "max_ind = np.argsort(cooc_vec)[-25:]\n",
        "\n",
        "# Plot the words and values\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.bar(np.arange(0, 25), cooc_vec[max_ind])\n",
        "plt.xticks(ticks=np.arange(0, 25), labels=[tokenizer.index_word[i] for i in max_ind], rotation=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ikKaX_r0NKEV",
        "outputId": "3545caa5-a2af-437f-92ff-29e41761cd08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7e6fa1deb520>,\n",
              "  <matplotlib.axis.XTick at 0x7e6fa1deb490>,\n",
              "  <matplotlib.axis.XTick at 0x7e6fa1e52b90>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df456c0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df46170>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df46c20>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df459c0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df477f0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df742e0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df74d90>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df75840>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df750c0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df76140>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df76bf0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df776a0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df47070>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df77fa0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df90a90>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df91540>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df91ff0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df91780>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df92a10>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df934c0>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9df93f70>,\n",
              "  <matplotlib.axis.XTick at 0x7e6f9dfa8a60>],\n",
              " [Text(0, 0, 'you'),\n",
              "  Text(1, 0, \"we've\"),\n",
              "  Text(2, 0, 'also'),\n",
              "  Text(3, 0, 'said'),\n",
              "  Text(4, 0, 'it'),\n",
              "  Text(5, 0, 'he'),\n",
              "  Text(6, 0, 'i'),\n",
              "  Text(7, 0, 'cas'),\n",
              "  Text(8, 0, 'understands'),\n",
              "  Text(9, 0, 'website'),\n",
              "  Text(10, 0, 'we'),\n",
              "  Text(11, 0, 'a'),\n",
              "  Text(12, 0, 'is'),\n",
              "  Text(13, 0, 'school'),\n",
              "  Text(14, 0, 'on'),\n",
              "  Text(15, 0, \"it's\"),\n",
              "  Text(16, 0, 'wales'),\n",
              "  Text(17, 0, 'that'),\n",
              "  Text(18, 0, ''),\n",
              "  Text(19, 0, 'of'),\n",
              "  Text(20, 0, 'in'),\n",
              "  Text(21, 0, 'and'),\n",
              "  Text(22, 0, 'for'),\n",
              "  Text(23, 0, 'the'),\n",
              "  Text(24, 0, 'bbc')])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe Algorithm\n",
        "### Defining Hyperparameters\n",
        "Here we define several hyperparameters including `batch_size` (amount of samples in a single batch) `embedding_size` (size of embedding vectors) `window_size` (context window size)."
      ],
      "metadata": {
        "id": "O9pdSNxyOg7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4096 # Data points in a single batch\n",
        "\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "\n",
        "window_size=1 # We use a window size of 1 on either side of target word\n",
        "\n",
        "epochs = 5 # Number of epochs to train for\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors\n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "# We sample valid datapoints randomly from a large window without always being deterministic\n",
        "valid_window = 250\n",
        "\n",
        "# When selecting valid examples, we select some of the most frequent words as well as\n",
        "# some moderately rare words as well\n",
        "np.random.seed(54321)\n",
        "random.seed(54321)\n",
        "\n",
        "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
        "valid_term_ids = np.append(\n",
        "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
        "    axis=0\n",
        ")"
      ],
      "metadata": {
        "id": "ioKJrA_aOxEL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Model Computations\n",
        "\n",
        "The model takes two inputs,\n",
        "\n",
        "* A (batch of) context word ID(s) - $i$\n",
        "* A (batch of) target word ID(s) - $j$\n",
        "\n",
        "and computes the following output,\n",
        "\n",
        "$w_i.\\tilde{w}_j + b_i + \\tilde{b}_j$\n",
        "\n",
        "where, $w_i$ is the context embeddings for the words in $i$, $\\tilde{w}_j$ is target embeddings for the words in $j$, $b_i$ and $\\tilde{b}_j$ are two separate biases for context and target spaces. Then the following loss function is used,\n",
        "\n",
        "$J = f(X_{ij}) \\sum_{i,j=1}^{V} (w_i\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij})^2$\n",
        "\n",
        "Here, X_{ij} is the value at (i,j) position in the co-occurrence matrix and f(X_{ij}) is a simple weighting function of X_{ij}. You can see that the loss function looks of the format,\n",
        "\n",
        "$J = A ( B - C ) ^ 2 $\n",
        "\n",
        "Therefore, we will use the mean-squared-error loss and feed in $f(X_{ij})$ values as sample weights during training."
      ],
      "metadata": {
        "id": "-wgEjaBLPFCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Input, Embedding, Dot, Add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# Define two input Layers for context and target words\n",
        "word_i = Input(shape=())\n",
        "word_j = Input(shape=())\n",
        "\n",
        "# Each context and target has their own embeddings (weights and biases)\n",
        "# Embedding weights\n",
        "embedding_i = Embedding(n_vocab, embedding_size, name='target_embedding')(word_i)\n",
        "embedding_j = Embedding(n_vocab, embedding_size, name='context_embedding')(word_j)\n",
        "\n",
        "# Embedding biases\n",
        "b_i = Embedding(n_vocab, 1, name='target_embedding_bias')(word_i)\n",
        "b_j = Embedding(n_vocab, 1, name='context_embedding_bias')(word_j)\n",
        "\n",
        "# Compute the dot product between embedding vectors (i.e. w_i.w_j)\n",
        "ij_dot = Dot(axes=-1)([embedding_i, embedding_j])\n",
        "\n",
        "# Add the biases (i.e. w_i.w_j + b_i + b_j)\n",
        "pred = Add()([ij_dot, b_i, b_j])\n",
        "\n",
        "# The final model\n",
        "glove_model = Model(inputs=[word_i, word_j], outputs=pred, name='glove_model')\n",
        "\n",
        "# Glove has a specific loss function with a sound mathematical\n",
        "# underpinning\n",
        "# It is a form of mean squared error\n",
        "glove_model.compile(loss=\"mse\", optimizer = 'adam')\n",
        "\n",
        "glove_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "degoeFYXO11h",
        "outputId": "ea020681-be59-4683-c8e7-06afe3a0587d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"glove_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " target_embedding (Embeddin  (None, 128)                  1920128   ['input_1[0][0]']             \n",
            " g)                                                                                               \n",
            "                                                                                                  \n",
            " context_embedding (Embeddi  (None, 128)                  1920128   ['input_2[0][0]']             \n",
            " ng)                                                                                              \n",
            "                                                                                                  \n",
            " dot (Dot)                   (None, 1)                    0         ['target_embedding[0][0]',    \n",
            "                                                                     'context_embedding[0][0]']   \n",
            "                                                                                                  \n",
            " target_embedding_bias (Emb  (None, 1)                    15001     ['input_1[0][0]']             \n",
            " edding)                                                                                          \n",
            "                                                                                                  \n",
            " context_embedding_bias (Em  (None, 1)                    15001     ['input_2[0][0]']             \n",
            " bedding)                                                                                         \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 1)                    0         ['dot[0][0]',                 \n",
            "                                                                     'target_embedding_bias[0][0]'\n",
            "                                                                    , 'context_embedding_bias[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3870258 (14.76 MB)\n",
            "Trainable params: 3870258 (14.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating data for GloVe model\n",
        "\n",
        "The Glove model we implemented,\n",
        "\n",
        "* Takes two inputs; context words and target words\n",
        "* Computes the mean squared error as, $(\\hat{y}_{ij} - log(X_{ij}))^2$ for the model output $\\hat{y}_{ij}$\n",
        "* Use sample weights returned by $f(X_{ij})$\n",
        "\n",
        "Therefore, in the data generator we return a tuple of,\n",
        "\n",
        "`(inputs, targets, sample weights)`\n",
        "\n",
        "which translates to,\n",
        "\n",
        "`((batch of target words, batch or context words), batch of log(X_{ij}), batch of f(X_{ij})`                                "
      ],
      "metadata": {
        "id": "tbSmYp9LQf7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_sequences = tokenizer.texts_to_sequences(news_stories)"
      ],
      "metadata": {
        "id": "_G9EuWK8QVd0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glove_data_generator(\n",
        "    sequences, window_size, batch_size, vocab_size, cooccurrence_matrix, x_max=100.0, alpha=0.75, seed=None\n",
        "):\n",
        "    \"\"\" Generate batches of inputs and targets for GloVe \"\"\"\n",
        "\n",
        "    # Shuffle the data so that, every epoch, the order of data is different\n",
        "    rand_sequence_ids = np.arange(len(sequences))\n",
        "    np.random.shuffle(rand_sequence_ids)\n",
        "\n",
        "    # We will use a sampling table to make sure, we don't oversample stopwords\n",
        "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "    # For ecach story/article\n",
        "    for si in rand_sequence_ids:\n",
        "      # Generate positive skip-grams while using sub-sampling\n",
        "      positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequences[si],\n",
        "          vocabulary_size=vocab_size,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0.0,\n",
        "          shuffle=False,\n",
        "          sampling_table=sampling_table,\n",
        "          seed=seed\n",
        "      )\n",
        "\n",
        "      # Take targets and context words separately\n",
        "      targets, context = zip(*positive_skip_grams)\n",
        "      targets, context = np.array(targets).ravel(), np.array(context).ravel()\n",
        "\n",
        "      x_ij = np.array(cooccurrence_matrix[targets, context].toarray()).ravel()\n",
        "\n",
        "      # Compute log - Introducing an additive shift to make sure we\n",
        "      # don't compute log(0)\n",
        "      log_x_ij = np.log(x_ij + 1)\n",
        "\n",
        "      # Sample weights\n",
        "      # if x < x_max => (x/x_max)**alpha / else => 1\n",
        "      sample_weights = np.where(x_ij < x_max, (x_ij/x_max)**alpha, 1)\n",
        "\n",
        "      # If seed is not provided generate a random one\n",
        "      if not seed:\n",
        "        seed = random.randint(0, 10e6)\n",
        "\n",
        "      # Shuffle data\n",
        "      np.random.seed(seed)\n",
        "      np.random.shuffle(context)\n",
        "      np.random.seed(seed)\n",
        "      np.random.shuffle(targets)\n",
        "      np.random.seed(seed)\n",
        "      np.random.shuffle(log_x_ij)\n",
        "      np.random.seed(seed)\n",
        "      np.random.shuffle(sample_weights)\n",
        "\n",
        "      # Generate a batch or data in the format\n",
        "      # ((target words, context words), log(X_ij) <- true targets, f(X_ij) <- sample weights)\n",
        "      for eg_id_start in range(0, context.shape[0], batch_size):\n",
        "        yield(\n",
        "            targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])],\n",
        "            context[eg_id_start: min(eg_id_start+batch_size, context.shape[0])]\n",
        "        ), log_x_ij[eg_id_start: min(eg_id_start+batch_size, log_x_ij.shape[0])], \\\n",
        "        sample_weights[eg_id_start: min(eg_id_start+batch_size, sample_weights.shape[0])]\n",
        "\n",
        "# Generate some data\n",
        "news_glove_data_gen = glove_data_generator(\n",
        "    news_sequences, 2, 10, n_vocab, cooc_mat\n",
        ")\n",
        "\n",
        "for x, y, z in news_glove_data_gen:\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(z)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-BQXszaSesX",
        "outputId": "92234170-b314-42c3-bac1-1352e119e316"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([ 684, 4599, 3841,  671, 1270,  897,  289, 2855, 8518, 1616]), array([7656, 4988, 4417,    9,    6, 1821, 1287,    3,  395, 3238]))\n",
            "[1.3862944 0.6931472 0.        1.7917595 2.3978953 2.7725887 3.2580965\n",
            " 1.3862944 0.        3.3672957]\n",
            "[0.07208434 0.03162277 0.         0.10573713 0.17782794 0.24102853\n",
            " 0.35355338 0.07208434 0.         0.38491827]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model\n",
        "\n",
        "Here we train the GloVe model we defined above. We train for `epochs` and at the end of each epoch, we compute word similarities on a set of chosen validation words (`valid_term_ids`). Similar to in Chapter 3, we use a Keras callback to compute the most similar words.\n",
        "\n",
        "### Calculating Word Similarities\n",
        "\n",
        "We calculate the similarity between two given words in terms of the cosine distance. To do this efficiently we use matrix operations to do so, as shown below. Furthermore, we define the computations as a callback, which will automatically run at the end of an epoch during model training."
      ],
      "metadata": {
        "id": "xfJXO4gjU3tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValidationCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
        "        self.valid_term_ids = valid_term_ids\n",
        "        self.model_with_embeddings = model_with_embeddings\n",
        "        self.tokenizer = tokenizer\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\" Validation logic \"\"\"\n",
        "\n",
        "        # We will use context embeddings to get the most similar words\n",
        "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
        "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
        "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
        "\n",
        "        # Get the embeddings corresponding to valid_term_ids\n",
        "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
        "\n",
        "        # Compute the similarity between valid_term_ids and all the embeddings\n",
        "        # V x d (d x D) => V x D\n",
        "        top_k = 5 # Top k items will be displayed\n",
        "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
        "\n",
        "        # Invert similarity matrix to negative\n",
        "        # Ignore the first one because that would be the same word as the probe word\n",
        "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
        "\n",
        "        # Print the output\n",
        "        for i, term_id in enumerate(valid_term_ids):\n",
        "\n",
        "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j > 1])\n",
        "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
        "\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "eP7jvdD1UwZk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_validation_callback = ValidationCallback(valid_term_ids, glove_model, tokenizer)\n",
        "\n",
        "# Train the model for several epochs\n",
        "for ei in range(epochs):\n",
        "  print(\"Epoch: {}/{} started\".format(ei+1, epochs))\n",
        "\n",
        "  news_glove_data_gen = glove_data_generator(news_sequences, window_size, batch_size, n_vocab, cooc_mat)\n",
        "  glove_model.fit(news_glove_data_gen, epochs=1, callbacks=glove_validation_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQgCyB5xVkfF",
        "outputId": "0f837837-6a06-4457-905e-a3ee4a467295"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 started\n",
            "   2221/Unknown - 134s 58ms/step - loss: 0.6043election: attorney, poster, labour's, advertising\n",
            "months: weeks, years, days, times, or\n",
            "with: through, for, together, strike, government's\n",
            "you: we, they, only, now, there\n",
            "were: are, but, when, where, was\n",
            "win: put, technology, players, which\n",
            "those: work, any, many, released, part\n",
            "music: us, many, china, content\n",
            "also: it, now, there, become, only\n",
            "third: second, first, fifth, or, 15\n",
            "best: actress, category, association, shares, any\n",
            "down: government's, helped, strong, specific, off\n",
            "too: bigger, so, anticipated, go, edinburgh\n",
            "some: any, released, data, my, deal\n",
            "through: over, after, good, get, first\n",
            "mr: mikhail, tony, jack, bernie, gordon\n",
            "unit: running, one, any, against, or\n",
            "add: statistics, board, kill, airlines, computers\n",
            "trading: thursday, then, another, released, place\n",
            "along: work, time, part, the, her\n",
            "positive: but, after, good, government, all\n",
            "captain: urban, excellent, and, against, after\n",
            "written: just, a, put, means\n",
            "details: china, commons, released, challenge\n",
            "giving: go, main, second, first, things\n",
            "ruled: large, own, pirated, pension, total\n",
            "deutsche: kidney, scarlets, logical, robert, honorary\n",
            "loss: all, first, second, whole, win\n",
            "j: jon, joe, spend, i, be\n",
            "alan: mr, book, forum, ed, goal\n",
            "response: each, board, one, number, arrest\n",
            "illegal: over, and, higher, just, or\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 134s 58ms/step - loss: 0.6036\n",
            "Epoch: 2/5 started\n",
            "   2220/Unknown - 36s 16ms/step - loss: 0.0378election: attorney, motors, november's, poster, labour's\n",
            "months: weeks, days, years, past, matches\n",
            "with: bbc's, honoured, priorities, sponsorship, breach\n",
            "you: we, they, afford, understand, know\n",
            "were: when, are, where, say, but\n",
            "win: start, deal, him, end, increase\n",
            "those: artists, put, because, risk, work\n",
            "music: subscriber, divide, cameras, revolution, movies\n",
            "also: become, now, which, play, going\n",
            "third: fifth, fourth, second, short, takes\n",
            "best: actress, category, actor, supporting, due\n",
            "down: isn't, alone, catalogue, instead, helped\n",
            "too: bigger, anticipated, find, group's, how\n",
            "some: deal, developments, important, way, game\n",
            "through: around, government's, among, might, produce\n",
            "mr: 63, bernie, malcolm, mikhail, ken\n",
            "unit: one, running, number, total, important\n",
            "add: statistics, board, echoed, contrast, looking\n",
            "trading: thursday, deal, farmers, then, 16\n",
            "along: deal, work, december, round, role\n",
            "positive: way, game, after, head, government\n",
            "captain: 31, excellent, urban, seeking, group's\n",
            "written: deal, way, me, comes, work\n",
            "details: comes, movies, impact, investigations, released\n",
            "giving: intercept, go, further, things, take\n",
            "ruled: own, coming, ticket, owned, pirated\n",
            "deutsche: shareholder, austria, austria's, staff, peston's\n",
            "loss: system, officer, whole, problem, end\n",
            "j: jon, birkett, replacing, joe, cartilage\n",
            "alan: mr, 63, malcolm, bernie, ed\n",
            "response: each, u2's, awarded, millions, homes\n",
            "illegal: watching, steroids, pirated, finnish, uses\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 37s 16ms/step - loss: 0.0378\n",
            "Epoch: 3/5 started\n",
            "   2223/Unknown - 38s 17ms/step - loss: 0.0160election: attorney, poster, motors, november's, selected\n",
            "months: weeks, days, months', live, past\n",
            "with: breach, relaxation, bbc's, honoured, reopen\n",
            "you: we, they, afford, everybody, asked\n",
            "were: are, where, when, say, was\n",
            "win: start, increase, reach, end, contribution\n",
            "those: because, ensure, revealed, accused, saying\n",
            "music: divide, cameras, subscriber, refuseniks, asymmetric\n",
            "also: become, play, mean, hoping, forced\n",
            "third: fourth, long, fifth, short, takes\n",
            "best: actor, honda, actress, category, supporting\n",
            "down: microphone, catalogue, nem, searchers, 2009\n",
            "too: bigger, larger, find, voters', fanfare\n",
            "some: types, result, worsened, lot, series\n",
            "through: government's, door, comments, together, tough\n",
            "mr: 63, bernie, mikhail, article, wes\n",
            "unit: volume, ruled, began, side, latest\n",
            "add: statistics, decide, drawn, looking, facing\n",
            "trading: thursday, contact, fear, emergency, itv1\n",
            "along: deal, favour, driven, least, october\n",
            "positive: rigorous, way, during, elimination, conservatives\n",
            "captain: 31, telecom's, undisclosed, extra, construction\n",
            "written: comes, came, me, along, transport\n",
            "details: along, comes, soon, frost, tiebreak\n",
            "giving: intercept, further, miss, presley's, ict\n",
            "ruled: carried, ticket, own, find, miss\n",
            "deutsche: austria, austria's, shareholder, central, peston's\n",
            "loss: whole, problem, rest, limit, way\n",
            "j: birkett, jon, byrne, wasps, sweden's\n",
            "alan: mr, bernie, 63, ed, malcolm\n",
            "response: him, homes, order, start, freedom\n",
            "illegal: steroids, uses, construction, watching, stoke\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 38s 17ms/step - loss: 0.0159\n",
            "Epoch: 4/5 started\n",
            "   2222/Unknown - 30s 13ms/step - loss: 0.0114election: attorney, november's, motors, poster, campbell's\n",
            "months: weeks, certificate, reissues, days, prizes\n",
            "with: relaxation, breach, honoured, bbc's, reopen\n",
            "you: we, afford, they, god, happen\n",
            "were: are, where, was, say, when\n",
            "win: homes, victory, worried, talking, halt\n",
            "those: because, where, starred, wants, bought\n",
            "music: divide, cameras, refuseniks, creatures, unlimited\n",
            "also: become, forced, failed, continue, play\n",
            "third: fourth, long, replay, fifth, takes\n",
            "best: honda, category, actor, supporting, heirs\n",
            "down: downs, microphone, stubbornly, 2009, catalogue\n",
            "too: fanfare, bigger, stronger, larger, better\n",
            "some: rest, stage, accused, thousands, lot\n",
            "through: door, to, tough, fatal, comments\n",
            "mr: 63, bernie, cherie, tony, mikhail\n",
            "unit: raises, crop, increased, monumental, stockpile\n",
            "add: statistics, asked, echoed, tea, set\n",
            "trading: cpl, saturday, thursday, farmers, vanity\n",
            "along: cope, dealt, favour, deal, homes\n",
            "positive: during, rest, rigorous, way, indication\n",
            "captain: 31, telecom's, undisclosed, reacts, secondary\n",
            "written: answer, upcoming, along, intervention, laundering\n",
            "details: along, soon, sum, shortly, cope\n",
            "giving: intercept, further, presley's, tap, proposed\n",
            "ruled: carried, pointed, pulled, login, pull\n",
            "deutsche: austria, austria's, shareholder, central, accounts\n",
            "loss: rest, whole, wider, result, responsibilities\n",
            "j: birkett, jon, wasps, sweden's, byrne\n",
            "alan: mr, ousting, ameobi, bernie, 63\n",
            "response: chance, bring, homes, ensure, freedom\n",
            "illegal: steroids, uses, estimating, stoke, leaflets\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 30s 14ms/step - loss: 0.0114\n",
            "Epoch: 5/5 started\n",
            "   2221/Unknown - 25s 11ms/step - loss: 0.0093election: attorney, november's, labour's, motors, guides\n",
            "months: weeks, trophies, reissues, years, certificate\n",
            "with: reservoir, relaxation, bbc's, honoured, reopen\n",
            "you: afford, we, they, god, goodness\n",
            "were: are, was, saying, makes, timed\n",
            "win: victory, halt, recover, talk, reach\n",
            "those: because, dealer, maintains, bend, died\n",
            "music: divide, cameras, refuseniks, subscriber, mp3\n",
            "also: currently, mendes, trigger, become, continue\n",
            "third: fourth, fifth, long, short, takes\n",
            "best: heirs, category, honda, supporting, actor\n",
            "down: alone, downs, merely, stubbornly, assurances\n",
            "too: fanfare, larger, bigger, stronger, interference\n",
            "some: rest, sort, accused, kinds, series\n",
            "through: door, to, punters, advantage, bickering\n",
            "mr: bernie, kiely, cherie, 63, tony\n",
            "unit: stockpile, forthcoming, hamstring, successive, parts\n",
            "add: statistics, set, drawn, signed, wake\n",
            "trading: cpl, vanity, thursday, contact, pursuit\n",
            "along: cope, dealt, compared, favour, 2010\n",
            "positive: rigorous, during, catch, who's, conservatives\n",
            "captain: reacts, 31, telecom's, voters', secondary\n",
            "written: answer, dealt, thousands, along, outlines\n",
            "details: along, sum, soon, frost, impact\n",
            "giving: intercept, further, tap, presley's, proposed\n",
            "ruled: carried, pointed, pulled, singled, sorted\n",
            "deutsche: austria, austria's, central, shareholder, prediction\n",
            "loss: enhancing, topped, handful, problem, system\n",
            "j: jon, birkett, sa, o, sweden's\n",
            "alan: mr, bernie, ameobi, cherie, malcolm\n",
            "response: chance, bring, failed, hopes, ensure\n",
            "illegal: steroids, grokster, estimating, rasheed, ripping\n",
            "\n",
            "\n",
            "2225/2225 [==============================] - 25s 11ms/step - loss: 0.0093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the embeddings\n",
        "We save the learned embeddings to the disk"
      ],
      "metadata": {
        "id": "dE3-sORrXw0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
        "\n",
        "    # Create the directory if doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Get the words sorted according to their ID from the tokenizer\n",
        "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
        "    # Add one word in front to represent the reserved ID (0)\n",
        "    words_sorted = [None] + list(words_sorted)\n",
        "\n",
        "    # Create a new array by concatenating embeddings and bias\n",
        "\n",
        "    context_embedding_weights = model.get_layer(\"context_embedding\").get_weights()[0]\n",
        "    context_embedding_bias = model.get_layer(\"context_embedding_bias\").get_weights()[0]\n",
        "    context_embedding = np.concatenate([context_embedding_weights, context_embedding_bias], axis=1)\n",
        "\n",
        "    target_embedding_weights = model.get_layer(\"target_embedding\").get_weights()[0]\n",
        "    target_embedding_bias = model.get_layer(\"target_embedding_bias\").get_weights()[0]\n",
        "    target_embedding = np.concatenate([target_embedding_weights, target_embedding_bias], axis=1)\n",
        "\n",
        "    # Save the array as a Pandas DataFrames\n",
        "    pd.DataFrame(\n",
        "        context_embedding,\n",
        "        index = words_sorted\n",
        "    ).to_pickle(os.path.join(save_dir, \"context_embedding_and_bias.pkl\"))\n",
        "\n",
        "    pd.DataFrame(\n",
        "        target_embedding,\n",
        "        index = words_sorted\n",
        "    ).to_pickle(os.path.join(save_dir, \"target_embedding_and_bias.pkl\"))\n",
        "\n",
        "save_embeddings(glove_model, tokenizer, n_vocab, save_dir='glove_embeddings')"
      ],
      "metadata": {
        "id": "v7VNaAPoWax9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "L5Aw-fSZX470"
      }
    }
  ]
}